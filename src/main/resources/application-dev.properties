#############################
#    ENTRADA configuration  #
#############################

# This is the configuration file for ENTRADA. Most of the options in this file use sensible
# defaults and do not need to be changed.
# To get started with ENTRADA make sure to configure the following options 
# Options can also bet set using environment variables.
#
# 1) Setup ENTRADA for local, aws or hadoop (choose 1), option:
#
#     	entrada.engine
#
# 2) Configure data locations, options:
#
#     	entrada.location.work
#     	entrada.location.input
#     	entrada.location.output
#     	entrada.location.archive
#
#
# 3) Add name servers that have to be processed, option:
#
#		entrada.nameservers
#
#
# 4) Database, ENTRADA requires a database to store information about processed
#    PCAP files and table partitions. H2 and PostgreSQL are supported, enable one only, option:
#
#		entrada.nameservers

#########################
#    Data Locations     #
#########################

#work (always use local disk)
entrada.location.work=/tmp/entrada/data/work
entrada.location.log=/tmp/entrada/data/log
entrada.location.conf=/tmp/entrada/data/conf

#Local
entrada.location.input=/tmp/entrada/data/input
entrada.location.output=/tmp/entrada/data/output
entrada.location.archive=/tmp/entrada/data/archive

#Hadoop HDFS
#entrada.location.input=${hdfs.nameservice}/user/entrada-test/input
#entrada.location.output=${hdfs.nameservice}/user/entrada-test/database
#entrada.location.archive=${hdfs.nameservice}/user/entrada-test/archive

#Amazon S3
#entrada.location.input=s3://${aws.bucket}/input/
#entrada.location.output=s3://${aws.bucket}/database/
#entrada.location.archive=s3://${aws.bucket}/archive


#########################
#       Database        #
#########################

# Choose between PostgreSQL or H2 database
# Make sure that all options for required database are uncommented
# and that all options for the unused database are commented out.

#########################
#      PostgreSQL       #
#########################
# Make sure a PostgreSQL database with the correct username and password have been created
# before starting ENTRADA.
spring.datasource.url=
spring.datasource.username=
spring.datasource.password=
spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect
spring.datasource.driver-class-name=org.postgresql.Driver


#########################
#          H2           #
#########################
# H2 will create a database file in the work directory.
# NOTE: when updating ENTRADA and using H2, make sure the old 
# H2 database files are not deleted.

#spring.datasource.url=jdbc:h2:file:${entrada.location.work}/entrada.db;MODE=PostgreSQL
#spring.datasource.driver-class-name=org.h2.Driver
#spring.datasource.username=sa
#spring.datasource.password=entrada
#spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
#spring.h2.console.enabled=true
#spring.h2.console.path=/h2

# 
# Database JPA options
spring.jpa.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false
spring.jpa.generate-ddl=false
spring.jpa.properties.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.hbm2ddl.auto=none
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false
spring.jpa.properties.hibernate.id.new_generator_mappings=true
#logging.level.org.hibernate.SQL=DEBUG
#logging.level.org.hibernate.type=TRACE
spring.jpa.show-sql=false
spring.jpa.hibernate.format_sql=false

#
# Connectionpool options
# for Hikari connectionpool options see see: https://github.com/brettwooldridge/HikariCP
spring.jpa.hibernate.connection.provider_class=org.hibernate.hikaricp.internal.HikariCPConnectionProvider
spring.datasource.hikari.maximumPoolSize=20
# remove idle conns after 5 minutes
spring.datasource.hikari.idleTimeout=300000
spring.datasource.hikari.poolName=SpringBootJPAHikariCP
# maxlife each conn is 10 min
spring.datasource.hikari.maxLifetime=600000
# wait max 10sec when creating conn
spring.datasource.hikari.connectionTimeout=10000
spring.datasource.hikari.test-on-borrow=true
spring.datasource.hikari.validation-query=SELECT 1

# Flyway is used for versioning and deployment of the ENTRADA database schema
spring.flyway.baseline-on-migrate=true
spring.flyway.validate-on-migrate=false
logging.level.org.flywaydb=INFO


#########################
#        ENTRADA        #
#########################
# entrada.location.output must match the engine
# e.g.  engine == hadoop && entrada.location.output == hdfs://...
entrada.engine=local
# List of name server sub-directories in the inout directory
# each server sub-directories can have format <ns>_<anycast_site>
# the ns and anycast_site parts will be extracted and save with the DNS data
entrada.nameservers=
# name of the entrada database and tables that should be created 
entrada.database.name=entradatest
entrada.database.table.dns=dns
entrada.database.table.icmp=icmp
# skip the last pcap file in the input dir, this file might still be
# written to if rsync is used.
entrada.input.file.skipfirst=false
# execute the pcap processing every minute unless already running
entrada.execution.delay=60
# max number of rows per parquet output file
entrada.parquet.max=3000000
# minutes before dns cache entries timeout
# required to match dns requests spanning multiple pcap files
# time should be > number of recorded minutes in each pcap 
entrada.cache.timeout=10
# minutes before tcp flows cache entries timeout
# expired tcp flows are discarded
# time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.tcp.flows=1
# minutes before fragmented ip cache entries timeout
# expired fragments are discarded
# time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.ip.fragmented=1
# pcap inputstream buffer in kilobytes
entrada.inputstream.buffer=64
# enable processing of ICMP packets
entrada.icmp.enable=false
# Archive mode can be any of: archive, delete, none
# archive: move pcap file to acchive location
# delete: delete pcap file
# none: no action taken 
entrada.pcap.archive.mode=archive
# enable/disable compaction ( merge may small files into fewer lager files)
entrada.parquet.compaction.enabled=true
# check every x minutes if there are partitions that need to be compacted
entrada.parquet.compaction.interval=1
# the number of minutes to wait after the last data has been written to a partition
# before compaction of the partition is performed.
# The partition for the current day is not compacted until the next day + ${entrada.parquet.compaction.interval}
entrada.parquet.compaction.age=1
# milliseconds writer thread will wait if no packets are available in queue to process 
entrada.queue.empty.wait=100
# schedule for maintenance 
entrada.maintenance.interval=5
# max number of days to keep row in entrada_file_archive table
entrada.database.files.max.age=1
# max number of days to keep archived pcap files
entrada.archive.files.max.age=1

#########################
#        Resolvers      #
#########################

# used to retrieve subnets of Google Public Resolver. dig TXT locations.publicdns.goog.
google.resolver.hostname=locations.publicdns.goog.
# timeout in secs
google.resolver.timeout=15

# where to retrieve subnets of OpenDNS resolver
opendns.resolver.url=https://www.opendns.com/network-map-data
# timeout in secs
opendns.resolver.timeout=15

# where to retrieve subnets of CloudFlare resolver
cloudflare.resolver.ipv4.url=https://www.cloudflare.com/ips-v4
cloudflare.resolver.ipv6.url=https://www.cloudflare.com/ips-v6
# timeout in secs
cloudflare.resolver.timeout=15

# quad9 subnets are read from embedded resource file

#########################
#          AWS          #
#########################

# bucket that should be used by entrada
# if the bucket does not exist it will be created automatically
# it will be created  with encryption enabled and non-public access by default
aws.bucket=entrada-data-sidn-test-2
# use AWS S3 encryption
aws.encryption=true
# Disable Spring Boot Cloudformation in Spring Cloud AWS
# Needs to be false when not running on EC2
cloud.aws.stack.auto=false
# the S3 region to use
cloud.aws.region.static=eu-west-1
# spring cloud config for aws
# see: https://cloud.spring.io/spring-cloud-static/spring-cloud-aws/2.0.0.RELEASE/multi/multi__basic_setup.html#_configuring_credentials
cloud.aws.credentials.useDefaultAwsCredentialsChain=true
# minimum size of each file part when uploading to s3 (multipart uploading)
aws.upload.multipart.mb.size=5
# number of threads to use when uploading to S3
aws.upload.parallelism=10
# storage class for parquet files uploaded to S3 bucket
aws.upload.upload.storage-class=STANDARD_IA
# storage class for pcap files uploaded to S3 bucket
aws.upload.archive.storage-class=STANDARD_IA

aws.bucket.rules.url=https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html


# Athena
# Athena workgroup to use
athena.workgroup=primary
athena.driver.name=com.simba.athena.jdbc.Driver
athena.url=jdbc:awsathena://AwsRegion=${cloud.aws.region.static};
# where Athena will store the results of queries
athena.output.location=s3://${aws.bucket}/entrada-athena-output/
# how many days to keep the Athena query results, this is used to create
# a bucket lifecycle for ${athena.output.location} all data will be deleted
# by S3 after the x of days
athena.output.expiration=4
# Athena logging, will only be enabled when log4j is set to debug level
athena.log.level=4
athena.log.path=${entrada.location.work}/athena-logs/


#########################
#        Hadoop         #
#########################

# hostname of the Hadoop name node
hdfs.nameservice.host=
# hostname of a Impala daemon node to send Impala queries DDL queries to 
impala.daemon.host=
# HDFS nameservice URL
hdfs.nameservice=hdfs://${hdfs.nameservice.host}:8020
#hadoop user to use
hdfs.username=hdfs
#set the owner/group for the parquet files on HDFS, Impala must have write permission
hdfs.data.owner=impala
hdfs.data.group=hive

# all Impala connection options are set using the impala.url option
# see https://www.cloudera.com/documentation/other/connectors/impala-jdbc/latest/Cloudera-JDBC-Driver-for-Impala-Install-Guide.pdf
# examples:
#
#	no authentication:
#		impala.url=jdbc:impala://${IMPALA_NODE}:21050;AuthMech=0;
#
#	Kerberos authentication:
#		 Add the following JVM args for Kerberos config and JAAS config:
#   		-Djava.security.krb5.conf=/path/to/krb5.conf
#			-Djava.security.auth.login.config=/path/to/jaas.conf
#
#
# 
# Kerberos username e.g. 
#kerberos.username=
# name of the Kerberos realm to use
kerberos.realm=
# Path to the Kerberos keytab file
kerberos.keytab=

# Impala JDBC connection url.
# Uses the above mention options to create a Impala URL
kerberos.impala.url=jdbc:impala://${impala.daemon.host}:21050;AuthMech=1;KrbRealm=${kerberos.realm};KrbHostFQDN=${impala.daemon.host};KrbServiceName=impala


#########################
#      MaxMind GeoIP    #
#########################
# location of MaxMind database files
geoip.maxmind.location=${entrada.location.work}/maxmind
# max number of days to use db before new db version should be downloaded
# this check is only performed during startup
geoip.maxmind.age.max=30
geoip.maxmind.url.country=https://geolite.maxmind.com/download/geoip/database/GeoLite2-Country.tar.gz
geoip.maxmind.url.asn=https://geolite.maxmind.com/download/geoip/database/GeoLite2-ASN.tar.gz
geoip.maxmind.url.country.paid=https://download.maxmind.com/app/geoip_download?edition_id=GeoIP2-Country&suffix=tar.gz&license_key=
geoip.maxmind.url.asn.paid=https://download.maxmind.com/app/geoip_download?edition_id=GeoIP2-ISP&suffix=tar.gz&license_key=
# subscription license key 
geoip.maxmind.subsciption.key=


#########################
#        Metrics        #
#########################
management.metrics.export.graphite.enabled=true
# Metrics to send to Graphite/Grafana to monitor ENTRADA
management.metrics.export.graphite.host=
management.metrics.export.graphite.port=2003
management.metrics.export.graphite.protocol=Plaintext
management.metrics.export.graphite.step=10s
management.metrics.export.graphite.tags-as-prefix=prefix
management.metrics.export.graphite.prefix=entrada-test
# retention agregation, default 10sec
# all metrics are grouped into 10s buckets
# http://graphite.readthedocs.org/en/latest/config-carbon.html#storage-schemas-conf
management.metrics.export.graphite.retention=10
# enable/disable standard metrics
management.metrics.enable.jvm=true
management.metrics.enable.process=true
management.metrics.enable.system=true
management.metrics.enable.tomcat=false
management.metrics.enable.hikaricp=false
management.metrics.enable.jdbc=false
management.metrics.enable.logback=false

management.endpoint.metrics.enabled=false
#management.endpoints.web.exposure.include=*

#########################
#        Logging        #
#########################
# set log level
logging.level.nl.sidnlabs=DEBUG
logging.level.com.amazonaws=INFO
logging.path=${entrada.location.log}

#########################
#        Web server     #
#########################
# what port should ENTRADA listen to.
server.port=8080
server.servlet.context-path=/api/v2
