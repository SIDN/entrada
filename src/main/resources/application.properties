#############################
#    ENTRADA configuration  #
#############################

# This is the configuration file for ENTRADA. Most of the options in this file use sensible
# defaults and do not need to be changed.
# To get started with ENTRADA make sure to configure the following options 
# Options can also bet set using environment variables.
#
# 1) Setup ENTRADA for local, aws or hadoop (choose 1), option:
#
#     	entrada.engine
#
# 2) Configure data locations, options:
#
#     	entrada.location.work
#     	entrada.location.input
#     	entrada.location.output
#     	entrada.location.archive
#
#
# 3) Add name servers that have to be processed, option:
#
#		entrada.nameservers
#
#
# 4) Database, ENTRADA requires a database to store information about processed
#    PCAP files and table partitions. H2 and PostgreSQL are supported, enable one only, option:
#
#		entrada.nameservers

#########################
#    Data Locations     #
#########################

entrada.location.conf=/entrada/data/conf
entrada.location.log=/entrada/data/log
#work (always use local disk)
entrada.location.work=/entrada/data/work
#separate the volatile from the non-volatile data (allow for IO optimisation / mountpoints)
entrada.location.persistence=${entrada.location.work}

#Local
entrada.location.input=/entrada/data/input
entrada.location.output=/entrada/data/output
entrada.location.archive=/entrada/data/archive

#Hadoop HDFS
#entrada.location.input=${hdfs.nameservice}/user/entrada/pcap
#entrada.location.output=${hdfs.nameservice}/user/entrada/database
#entrada.location.archive=${hdfs.nameservice}/user/entrada/archive

#Amazon S3
#entrada.location.input=s3://${aws.bucket}/input/
#entrada.location.output=s3://${aws.bucket}/database/
#entrada.location.archive=s3://${aws.bucket}/archive


#########################
#       Database        #
#########################

# Choose between PostgreSQL or H2 database
# Make sure that all options for required database are uncommented
# and that all options for the unused database are commented out.

#########################
#      PostgreSQL       #
#########################
# Make sure a PostgreSQL database with the correct username and password have been created
# before starting ENTRADA.
#spring.datasource.url=
#spring.datasource.username=
#spring.datasource.password=
#spring.jpa.database-platform=org.hibernate.dialect.PostgreSQLDialect
#spring.datasource.driver-class-name=org.postgresql.Driver


#########################
#          H2           #
#########################
# H2 will create a database file in the work directory.
# NOTE: when updating ENTRADA and using H2, make sure the old 
# H2 database files are not deleted.

#spring.datasource.url=jdbc:h2:file:${entrada.location.work}/entrada.db
#spring.datasource.driver-class-name=org.h2.Driver
#spring.datasource.username=entrada
#spring.datasource.password=
#spring.jpa.database-platform=org.hibernate.dialect.H2Dialect
#spring.h2.console.enabled=true
#spring.h2.console.path=/h2

# 
# Database JPA options
spring.jpa.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false
spring.jpa.generate-ddl=false
spring.jpa.properties.hibernate.ddl-auto=none
spring.jpa.properties.hibernate.hbm2ddl.auto=none
spring.jpa.show-sql=false
spring.jpa.hibernate.format_sql=false
spring.jpa.properties.hibernate.temp.use_jdbc_metadata_defaults=false
spring.jpa.properties.hibernate.id.new_generator_mappings=true

#
# Connectionpool options
# for Hikari connectionpool options see see: https://github.com/brettwooldridge/HikariCP
# The database driver (Postgres or H2) will be autodetected based on connection url
spring.datasource.hikari.jdbcUrl=jdbc:h2:file:${entrada.location.work}/entrada.db;MODE=PostgreSQL
spring.datasource.hikari.username=entrada
spring.datasource.hikari.password=
spring.jpa.hibernate.connection.provider_class=org.hibernate.hikaricp.internal.HikariCPConnectionProvider
spring.datasource.hikari.minimumIdle=1
spring.datasource.hikari.maximumPoolSize=5
# remove idle conns after 5 minutes
spring.datasource.hikari.idleTimeout=300000
spring.datasource.hikari.poolName=SpringBootJPAHikariCP
# maxlife each conn is 10 min
spring.datasource.hikari.maxLifetime=600000
# wait max 10sec when creating conn
spring.datasource.hikari.connectionTimeout=10000
spring.datasource.hikari.test-on-borrow=true
spring.datasource.hikari.validation-query=SELECT 1

# Flyway is used for versioning and deployment of the ENTRADA database schema
spring.flyway.baseline-on-migrate=true
spring.flyway.validate-on-migrate=false
logging.level.org.flywaydb=INFO


#########################
#        ENTRADA        #
#########################
# is this node the master when running multiple
# instances using same database.
entrada.node.master=true
# entrada.location.output must match the engine
# e.g.  engine == hadoop && entrada.location.output == hdfs://...
entrada.engine=local
# List of name server sub-directories in the inout directory
# each server sub-directories can have format <ns>_<anycast_site>
# the ns and anycast_site parts will be extracted and save with the DNS data
entrada.nameservers=
# name of the entrada database and tables that should be created 
entrada.database.name=entrada
entrada.database.table.dns=dns
entrada.database.table.icmp=icmp
# skip the last pcap file in the input dir, this file might still be
# written to if rsync is used.
entrada.input.file.skipfirst=false
# execute the pcap processing every x seconds unless already running
entrada.execution.delay=60
# max number of rows per parquet output file
entrada.parquet.max=3000000
# minutes before dns cache entries timeout
# required to match dns requests spanning multiple pcap files
# time should be > number of recorded minutes in each pcap 
entrada.cache.timeout=10
# minutes before tcp flows cache entries timeout
# expired tcp flows are discarded
# time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.tcp.flows=10
# minutes before fragmented ip cache entries timeout
# expired fragments are discarded
# time should be > number of recorded minutes in each pcap 
entrada.cache.timeout.ip.fragmented=10
# pcap inputstream buffer in kilobytes
entrada.inputstream.buffer=64
# enable processing of ICMP packets
entrada.icmp.enable=true
# Archive mode can be any of: archive, delete, none
# archive: move pcap file to acchive location
# delete: delete pcap file
# none: no action taken 
entrada.pcap.archive.mode=archive
# enable/disable compaction ( merge may small files into fewer lager files)
entrada.parquet.compaction.enabled=true
# check every x minutes if there are partitions that need to be compacted
entrada.parquet.compaction.interval=5
# the number of minutes to wait after the last data has been written to a partition
# before compaction of the partition is performed.
# The partition for the current day is not compacted until the next day + ${entrada.parquet.compaction.interval}
entrada.parquet.compaction.age=60
# milliseconds writer thread will wait if no packets are available in queue to process 
entrada.queue.empty.wait=100
# schedule for maintenance 
entrada.maintenance.interval=3600
# max number of days to keep row in entrada_file_archive table
entrada.database.files.max.age=30
# max number of days to keep archived pcap files
entrada.archive.files.max.age=3
# when bulk processing files, make sure that the current partitions are not going to be compacted
# mark the partitions as "active" every x minutes
entrada.partition.activity.ping=5
# entrada privacy mode on/of, when true no src ip address related data is saved
entrada.privacy.enabled=false
# the number of days after which the src ip address must be removed from a stored partition
# this means rewriting all the parquet files and filtering out the src ips
# set to 0 to disable
entrada.privacy.purge.age=0
# check every x minutes if there are partitions that need to be purged of src ip address
entrada.privacy.purge.interval=10
# upload after all inout data has been processed (true) 
# or upload closed parquet files before all inout data is processed (false)
entrada.parquet.upload.batch=false

#########################
#        Resolvers      #
#########################

# max size of the "IP address to resolver" match cache
# keep cache because checking IP address is expensive
# but must make sure not to use a giant cache and get out-of-memory error
publicresolver.match.cache.size=5000

# used to retrieve subnets of Google Public Resolver. dig TXT locations.publicdns.goog.
google.resolver.hostname=locations.publicdns.goog.
# timeout in secs
google.resolver.timeout=15

# where to retrieve subnets of OpenDNS resolver
opendns.resolver.url=https://www.opendns.com/network-map-data
# timeout in secs
opendns.resolver.timeout=15

# where to retrieve subnets of CloudFlare resolver
cloudflare.resolver.ipv4.url=https://www.cloudflare.com/ips-v4
cloudflare.resolver.ipv6.url=https://www.cloudflare.com/ips-v6
# timeout in secs
cloudflare.resolver.timeout=15

# quad9 subnets are read from embedded resource file

#########################
#          AWS          #
#########################

# bucket that should be used by entrada
# if the bucket does not exist it will be created automatically
# it will be created  with encryption enabled and non-public access by default
aws.bucket=
# use AWS S3 encryption
aws.encryption=true
# Disable Spring Boot Cloudformation in Spring Cloud AWS
# Needs to be false when not running on EC2
cloud.aws.stack.auto=false
# the S3 region to use
cloud.aws.region.static=eu-west-1
# spring cloud config for aws
# see: https://cloud.spring.io/spring-cloud-static/spring-cloud-aws/2.0.0.RELEASE/multi/multi__basic_setup.html#_configuring_credentials
cloud.aws.credentials.useDefaultAwsCredentialsChain=true
# minimum size of each file part when uploading to s3 (multipart uploading)
aws.upload.multipart.mb.size=5
# number of threads to use when uploading to S3
aws.upload.parallelism=10
# storage class for parquet files uploaded to S3 bucket
aws.upload.upload.storage-class=STANDARD_IA
# storage class for pcap files uploaded to S3 bucket
aws.upload.archive.storage-class=STANDARD_IA

# manage S3 bucket and associated lifecycle rules from entrada code
aws.bucket.manage=true

aws.bucket.rules.url=https://docs.aws.amazon.com/AmazonS3/latest/dev/BucketRestrictions.html

# Athena
# Athena workgroup to use
athena.workgroup=primary
athena.driver.name=com.simba.athena.jdbc.Driver
athena.url=jdbc:awsathena://AwsRegion=${cloud.aws.region.static};
# where Athena will store the results of queries
athena.output.location=s3://${aws.bucket}/entrada-athena-output/
# how many days to keep the Athena query results, this is used to create
# a bucket lifecycle for ${athena.output.location} all data will be deleted
# by S3 after the x of days
athena.output.expiration=2
# Athena logging, will only be enabled when log4j is set to debug level
athena.log.level=4
athena.log.path=${entrada.location.work}/athena-logs/
# Athena QueryExecution polling overrides, to avoid getting rate limited by AWS
athena.polling.min=5
athena.polling.max=1000
athena.polling.multiplier=8

#########################
#        Hadoop         #
#########################

# hostname of the Hadoop name node
hdfs.nameservice.host=
# hostname of a Impala daemon node to send Impala queries DDL queries to 
impala.daemon.host=
# HDFS nameservice URL
hdfs.nameservice=hdfs://${hdfs.nameservice.host}
#hadoop user to use
hdfs.username=hdfs
#set the owner/group for the parquet files on HDFS, Impala must have write permission
hdfs.data.owner=impala
hdfs.data.group=hive

impala.log.level=1
impala.log.path=${entrada.location.log}/impala-logs/

# use ssl, make sure the root CA is in java cacerts file
impala.ssl=0

# additional url options
impala.url.additional

# all Impala connection options are set using the impala.url option
# see https://www.cloudera.com/documentation/other/connectors/impala-jdbc/latest/Cloudera-JDBC-Driver-for-Impala-Install-Guide.pdf
# examples:
#
#	no authentication:
impala.url=jdbc:impala://${impala.daemon.host}:21050;AuthMech=0;LogLevel=${impala.log.level};LogPath=${impala.log.path};SSL=${impala.ssl};${impala.url.additional}
#
#	Kerberos authentication:
#		 Add the following JVM args for Kerberos config and JAAS config:
#   		-Djava.security.krb5.conf=/path/to/krb5.conf
#			-Djava.security.auth.login.config=/path/to/jaas.conf
#
#
# 
# Kerberos username 
#kerberos.username=
# name of the Kerberos realm to use
kerberos.realm=
# Path to the Kerberos keytab file
kerberos.keytab=

# Impala JDBC connection url.
# Uses the above mention options to create a Impala URL
kerberos.impala.url=jdbc:impala://${impala.daemon.host}:21050;AuthMech=1;KrbRealm=${kerberos.realm};KrbHostFQDN=${impala.daemon.host};KrbServiceName=impala;LogLevel=${impala.log.level};LogPath=${impala.log.path};SSL=${impala.ssl};${impala.url.additional}


#########################
#      MaxMind GeoIP    #
#########################
# location of MaxMind database files
geoip.maxmind.location=${entrada.location.work}/maxmind
# max number of days to use db before new db version should be downloaded
# this check is only performed during startup
geoip.maxmind.age.max=30
#geolite free databases
geoip.maxmind.url.country=https://download.maxmind.com/app/geoip_download?edition_id=GeoLite2-Country&suffix=tar.gz&license_key=
geoip.maxmind.url.asn=https://download.maxmind.com/app/geoip_download?edition_id=GeoLite2-ASN&suffix=tar.gz&license_key=
# paid subscription
geoip.maxmind.url.country.paid=https://download.maxmind.com/app/geoip_download?edition_id=GeoIP2-Country&suffix=tar.gz&license_key=
geoip.maxmind.url.asn.paid=https://download.maxmind.com/app/geoip_download?edition_id=GeoIP2-ISP&suffix=tar.gz&license_key=
# subscription license key for free version
geoip.maxmind.license.free=
# subscription license key for paid version
geoip.maxmind.license.paid=

#########################
#        Metrics        #
#########################
management.metrics.export.graphite.enabled=false
# Metrics to send to Graphite/Grafana to monitor ENTRADA
management.metrics.export.graphite.host=
management.metrics.export.graphite.port=2003
management.metrics.export.graphite.protocol=Plaintext
management.metrics.export.graphite.step=10s
management.metrics.export.graphite.tags-as-prefix=prefix
management.metrics.export.graphite.prefix=entrada
# retention agregation, default 10sec
# all metrics are grouped into 10s buckets
# http://graphite.readthedocs.org/en/latest/config-carbon.html#storage-schemas-conf
management.metrics.export.graphite.retention=10
# enable/disable standard metrics
management.metrics.enable.jvm=false
management.metrics.enable.process=false
management.metrics.enable.system=false
management.metrics.enable.tomcat=false
management.metrics.enable.hikaricp=false
management.metrics.enable.jdbc=false
management.metrics.enable.logback=false

management.endpoint.metrics.enabled=false
#management.endpoints.web.exposure.include=*

#########################
#        Logging        #
#########################
# set log level
logging.level.nl.sidnlabs=INFO
logging.level.com.amazonaws=INFO
logging.path=${entrada.location.log}

#########################
#        Web server     #
#########################
# what port should ENTRADA listen to.
server.port=8080
#server.servlet.context-path=/api/v2
spring.data.rest.base-path=/api
